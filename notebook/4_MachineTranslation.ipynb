{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "import getpass\n",
    "\n",
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "path = \"/Users/{}/anaconda3/envs/rise_latest/etc/jupyter/nbconfig\".format(getpass.getuser())\n",
    "cm = BaseJSONConfigManager(config_dir=path)\n",
    "o = cm.update(\"livereveal\", {\n",
    "              \"theme\": \"sky\",\n",
    "              \"transition\": \"fade\",\n",
    "              \"start_slideshow_at\": \"selected\",\n",
    "})\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequence to Sequence Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topics\n",
    "* Encoder-Decoder Architecture\n",
    "* Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable Length Sequence to Sequence\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/Shape_of_NLP_Problems_8.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence-to-Sequence Overview\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/0_encoder_decoder.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoder-Decoder Architecture\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/1_encoder_decoder.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence-to-Sequence Training: Input\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/2_encoder_decoder.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence-to-Sequence Training: Loss\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/3_encoder_decoder.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence-to-Sequence Training: Teacher Forcing\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/3a_encoder_decoder.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence-to-Sequence Inference\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/4_encoder_decoder.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.nn import Linear, Embedding, RNN, GRU, LSTM\n",
    "from torch.nn import Sigmoid, LogSoftmax\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import BCELoss, NLLLoss, CrossEntropyLoss\n",
    "from string import punctuation\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11618, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600702</th>\n",
       "      <td>Allow us to state that the opening of the inqu...</td>\n",
       "      <td>Permita que le digamos que las investigaciones...</td>\n",
       "      <td>[&lt;SOS&gt;, allow, us, to, state, that, the, openi...</td>\n",
       "      <td>[&lt;SOS&gt;, permita, que, le, digamos, que, las, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>Since June of last year, the OLAF regulation h...</td>\n",
       "      <td>Desde junio del año pasado el Reglamento de la...</td>\n",
       "      <td>[&lt;SOS&gt;, since, june, of, last, year, the, olaf...</td>\n",
       "      <td>[&lt;SOS&gt;, desde, junio, del, año, pasado, el, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180999</th>\n",
       "      <td>I am fully aware of the views expressed here o...</td>\n",
       "      <td>Soy plenamente consciente de las opiniones exp...</td>\n",
       "      <td>[&lt;SOS&gt;, i, am, fully, aware, of, the, views, e...</td>\n",
       "      <td>[&lt;SOS&gt;, soy, plenamente, consciente, de, las, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109251</th>\n",
       "      <td>I will have clarification of that situation la...</td>\n",
       "      <td>Hoy mismo recibiré información sobre dicha sit...</td>\n",
       "      <td>[&lt;SOS&gt;, i, will, have, clarification, of, that...</td>\n",
       "      <td>[&lt;SOS&gt;, hoy, mismo, recibiré, información, sob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401751</th>\n",
       "      <td>Let us not forget, the Roma were the first to ...</td>\n",
       "      <td>No olvidemos que la población gitana fue la pr...</td>\n",
       "      <td>[&lt;SOS&gt;, let, us, not, forget, the, roma, were,...</td>\n",
       "      <td>[&lt;SOS&gt;, no, olvidemos, que, la, población, git...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   english  \\\n",
       "600702   Allow us to state that the opening of the inqu...   \n",
       "3733     Since June of last year, the OLAF regulation h...   \n",
       "1180999  I am fully aware of the views expressed here o...   \n",
       "109251   I will have clarification of that situation la...   \n",
       "1401751  Let us not forget, the Roma were the first to ...   \n",
       "\n",
       "                                                   spanish  \\\n",
       "600702   Permita que le digamos que las investigaciones...   \n",
       "3733     Desde junio del año pasado el Reglamento de la...   \n",
       "1180999  Soy plenamente consciente de las opiniones exp...   \n",
       "109251   Hoy mismo recibiré información sobre dicha sit...   \n",
       "1401751  No olvidemos que la población gitana fue la pr...   \n",
       "\n",
       "                                                      text  \\\n",
       "600702   [<SOS>, allow, us, to, state, that, the, openi...   \n",
       "3733     [<SOS>, since, june, of, last, year, the, olaf...   \n",
       "1180999  [<SOS>, i, am, fully, aware, of, the, views, e...   \n",
       "109251   [<SOS>, i, will, have, clarification, of, that...   \n",
       "1401751  [<SOS>, let, us, not, forget, the, roma, were,...   \n",
       "\n",
       "                                                     label  \n",
       "600702   [<SOS>, permita, que, le, digamos, que, las, i...  \n",
       "3733     [<SOS>, desde, junio, del, año, pasado, el, re...  \n",
       "1180999  [<SOS>, soy, plenamente, consciente, de, las, ...  \n",
       "109251   [<SOS>, hoy, mismo, recibiré, información, sob...  \n",
       "1401751  [<SOS>, no, olvidemos, que, la, población, git...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('../data/4_europarl_en_sp.pkl')\n",
    "data['text'] = data['english'].map(lambda x: \"\".join([i for i in x.lower() if i not in string.punctuation]).split())\n",
    "data['label'] = data['spanish'].map(lambda x: \"\".join([i for i in x.lower() if i not in string.punctuation]).split())\n",
    "\n",
    "data['text'] = data['text'].map(lambda x: ['<SOS>'] + x + ['<EOS>'])\n",
    "data['label'] = data['label'].map(lambda x: ['<SOS>'] + x + ['<EOS>'])\n",
    "print(data.shape)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "input_words = set(itertools.chain.from_iterable(data['text']))\n",
    "output_words = set(itertools.chain.from_iterable(data['label']))\n",
    "\n",
    "input2idx = {word: idx for idx, word in enumerate(input_words)}\n",
    "idx2input = {idx: word for word, idx in input2idx.items()}\n",
    "\n",
    "output2idx = {word: idx for idx, word in enumerate(output_words)}\n",
    "idx2output = {idx: word for word, idx in output2idx.items()}\n",
    "\n",
    "input_size = len(input_words)\n",
    "output_size = len(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "input_seqs = data['text'].map(lambda x: [input2idx[i] for i in x]).tolist()\n",
    "output_seqs = data['label'].map(lambda x: [output2idx[i] for i in x]).tolist()\n",
    "\n",
    "data = list(zip(input_seqs, output_seqs))\n",
    "\n",
    "train_data, test_data = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# First we'll want to create an embedding layer for our encoder\n",
    "enc_embedding = Embedding(num_embeddings=input_size, embedding_dim=100)\n",
    "\n",
    "# Next we'll want to create an LSTM for our encoder\n",
    "enc_rnn = LSTM(input_size=100, hidden_size=50)\n",
    "\n",
    "# Next we'll want to create an embedding layer for our decoder\n",
    "dec_embedding = Embedding(num_embeddings=output_size, embedding_dim=100)\n",
    "\n",
    "# Next, create an LSTM for the decoder\n",
    "dec_rnn = LSTM(input_size=100, hidden_size=50)\n",
    "\n",
    "# When we read the output from the decoder network we'll\n",
    "# want to classify it as one of the words from the output corpus\n",
    "# to do this, we'll need a linear layer\n",
    "dec_linear = Linear(50, output_size)\n",
    "\n",
    "# Lastly, we'll need an instance of LogSoftmax to convert the\n",
    "# output to a softmax distribution for feeding into NLLLoss\n",
    "# Hint: Set dim=0 on initialization here\n",
    "softmax = LogSoftmax(dim=1)\n",
    "\n",
    "# Create an instance of the NLLLoss\n",
    "criterion = NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence Tensor Shape: torch.Size([16])\n",
      "Output Sequence Tensor Shape: torch.Size([13])\n",
      "Encoder Embedded Sequence Shape: torch.Size([16, 100])\n",
      "Encoder Embedded Sequence Shape (1 batch): torch.Size([16, 100])\n",
      "Encoder Output Shape: torch.Size([16, 1, 50])\n",
      "Encoder Hidden Shape(s): torch.Size([1, 1, 50]) torch.Size([1, 1, 50])\n",
      "Decoder Embedded Sequence Shape: torch.Size([13, 100])\n",
      "Decoder Embedded Sequence Shape (1 batch): torch.Size([13, 100])\n",
      "Decoder LSTM Input Shape: torch.Size([12, 1, 100])\n",
      "Decoder Output Shape: torch.Size([12, 1, 50])\n",
      "Decoder Hidden Shape(s): torch.Size([1, 1, 50]) torch.Size([1, 1, 50])\n",
      "Decoder Linear Output Shape: torch.Size([12, 15608])\n",
      "Decoder Softmax Output Shape: torch.Size([12, 15608])\n",
      "Decoder Softmax Norms Shape: torch.Size([12])\n",
      "Decoder Softmax Norms: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000], grad_fn=<SumBackward2>)\n",
      "Decoder Loss Target Shape: torch.Size([12])\n",
      "Loss: tensor(9.7073)\n"
     ]
    }
   ],
   "source": [
    "# grab an example input and output sequence:\n",
    "input_seq = input_seqs[0]\n",
    "output_seq = output_seqs[0]\n",
    "\n",
    "# convert these to torch tensors\n",
    "input_seq_tensor = torch.LongTensor(input_seq)\n",
    "output_seq_tensor = torch.LongTensor(output_seq)\n",
    "print(\"Input Sequence Tensor Shape:\", input_seq_tensor.shape)\n",
    "print(\"Output Sequence Tensor Shape:\", output_seq_tensor.shape)\n",
    "\n",
    "# pass the input sequence through the encoder embedding\n",
    "enc_embedded = enc_embedding.forward(input_seq_tensor)\n",
    "print(\"Encoder Embedded Sequence Shape:\", enc_embedded.shape)\n",
    "\n",
    "# unsqueeze the embedding tensor to have a batch size of 1\n",
    "enc_embedded_unsqueezed = enc_embedded.unsqueeze(1)\n",
    "print(\"Encoder Embedded Sequence Shape (1 batch):\", enc_embedded.shape)\n",
    "\n",
    "# create initial hidden states for the encoder LSTM\n",
    "h0 = torch.zeros(1, 1, 50)\n",
    "c0 = torch.zeros(1, 1, 50)\n",
    "enc_hidden = (h0, c0)\n",
    "    \n",
    "\n",
    "# pass the embedded input sequence through the encoder LSTM\n",
    "enc_out, enc_hidden = enc_rnn(enc_embedded_unsqueezed, enc_hidden)\n",
    "print(\"Encoder Output Shape:\", enc_out.shape)\n",
    "print(\"Encoder Hidden Shape(s):\", enc_hidden[0].shape, enc_hidden[1].shape)\n",
    "\n",
    "# set the decoder rnn initial hidden state \n",
    "# to the last hidden state of the encoder rnn\n",
    "dec_hidden = enc_hidden\n",
    "\n",
    "# pass the output sequence through the decoder embedding layer\n",
    "dec_embedded = dec_embedding(output_seq_tensor)\n",
    "print(\"Decoder Embedded Sequence Shape:\", dec_embedded.shape)\n",
    "\n",
    "# unsqueeze the embedding tensor to have a batch size of 1\n",
    "dec_embedded_unsqueezed = dec_embedded.unsqueeze(1)\n",
    "print(\"Decoder Embedded Sequence Shape (1 batch):\", dec_embedded.shape)\n",
    "\n",
    "# Assuming only teacher forcing (running true answer as input to the decoder)\n",
    "# Run the output sequence through the dec LSTM\n",
    "# Note: we want to pass ALL BUT THE LAST elemebt of the output \n",
    "# sequence through the LSTM\n",
    "\n",
    "dec_lstm_in = dec_embedded_unsqueezed[:-1]\n",
    "print(\"Decoder LSTM Input Shape:\", dec_lstm_in.shape)\n",
    "\n",
    "dec_out, dec_hidden = dec_rnn(dec_lstm_in, dec_hidden)\n",
    "print(\"Decoder Output Shape:\", dec_out.shape)\n",
    "print(\"Decoder Hidden Shape(s):\", dec_hidden[0].shape, dec_hidden[1].shape)\n",
    "\n",
    "# Now we want to run the decoder output through our decoder linear layer\n",
    "# Also, squeeze the batch dimension (dim=1) of the linear output to get rid of it\n",
    "dec_linear_output = dec_linear(dec_out).squeeze(1)\n",
    "print(\"Decoder Linear Output Shape:\", dec_linear_output.shape)\n",
    "\n",
    "# pass the decoder linear output through a softmax\n",
    "dec_softmax_output = softmax(dec_linear_output)\n",
    "print(\"Decoder Softmax Output Shape:\", dec_softmax_output.shape)\n",
    "\n",
    "# verify that the decoder output distributions is _actually_ a softmax\n",
    "dec_softmax_norms = torch.exp(dec_softmax_output).sum(dim=1)\n",
    "print(\"Decoder Softmax Norms Shape:\", dec_softmax_norms.shape)\n",
    "print(\"Decoder Softmax Norms:\", dec_softmax_norms)\n",
    "\n",
    "# the targets for the loss function should be\n",
    "# ALL BUT THE FIRST element of the output sequence\n",
    "dec_loss_target = output_seq_tensor[1:]\n",
    "print(\"Decoder Loss Target Shape:\", dec_loss_target.shape)\n",
    "\n",
    "# Calculate the loss using the decoder softmax \n",
    "# output and the decoder loss target\n",
    "loss = criterion(dec_softmax_output, dec_loss_target)\n",
    "print(\"Loss:\", loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size):\n",
    "        super(encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = Embedding(num_embeddings=vocab_size, \n",
    "                                   embedding_dim=embedding_dim)\n",
    "        self.rnn = LSTM(input_size=embedding_dim, \n",
    "                       hidden_size=hidden_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.softmax = LogSoftmax()\n",
    "        self.hidden = self.init_hidden()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        e = self.embedding(x)\n",
    "        e = e.view(len(x), self.batch_size, -1)\n",
    "        out, self.hidden = self.rnn(e, self.hidden)\n",
    "        return out, self.hidden\n",
    "                  \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        c0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "class decoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, batch_size):\n",
    "        super(decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = Embedding(num_embeddings=vocab_size, \n",
    "                                   embedding_dim=embedding_dim)\n",
    "        self.rnn = LSTM(input_size=embedding_dim, \n",
    "                       hidden_size=hidden_dim)\n",
    "        self.linear = Linear(hidden_dim, output_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.softmax = LogSoftmax(dim=1)\n",
    "        self.hidden = self.init_hidden()\n",
    "                \n",
    "    def forward(self, input, hidden):\n",
    "        self.hidden = hidden\n",
    "        e = self.embedding(input)\n",
    "        e = e.view(len(input), self.batch_size, -1)\n",
    "        out, self.hidden = self.rnn(e, self.hidden)\n",
    "        self.out = out\n",
    "        output = self.linear(out[0])\n",
    "        so = self.softmax(output)\n",
    "        return so, self.hidden\n",
    "                  \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        c0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "class seq2seq(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "                \n",
    "    def forward(self, input_seq, output_seq, p_tf=0):\n",
    "        outputs = []\n",
    "        \n",
    "        self.enc.hidden = self.enc.init_hidden()\n",
    "        self.dec.hidden = self.dec.init_hidden()        \n",
    "        \n",
    "        enc_output, enc_hidden = enc.forward(torch.LongTensor(input_seq))\n",
    "        dec_hidden = enc_hidden\n",
    "        tf_cnt = 0\n",
    "        for i in range(output_seq.shape[0]):  \n",
    "            \n",
    "            if (np.random.uniform()) > p_tf and (i != 0):\n",
    "                dec_input = torch.LongTensor([torch.argmax(dec_output).data])\n",
    "            else:\n",
    "                dec_input = torch.LongTensor([output_seq[i]])\n",
    "                \n",
    "            dec_output, dec_hidden = self.dec.forward(dec_input, dec_hidden) \n",
    "            outputs.append(dec_output)\n",
    "            \n",
    "        return torch.stack(outputs).squeeze(1)\n",
    "    \n",
    "    def predict(self, input_seq, sos_idx, eos_idx, max_len=20):\n",
    "        outputs = []\n",
    "        self.enc.hidden = self.enc.init_hidden()\n",
    "        self.dec.hidden = self.dec.init_hidden()   \n",
    "        \n",
    "        enc_output, enc_hidden = enc.forward(torch.LongTensor(input_seq))\n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        cnt = 0\n",
    "        dec_input = torch.LongTensor([sos_idx])\n",
    "        \n",
    "        dec_output, dec_hidden = self.dec.forward(dec_input, dec_hidden) \n",
    "        \n",
    "        output_idx = torch.argmax(dec_output).data\n",
    "        \n",
    "        while (int(output_idx) != eos_idx) and (cnt <= max_len): \n",
    "            cnt += 1\n",
    "            dec_input = torch.LongTensor([output_idx])        \n",
    "            dec_output, dec_hidden = self.dec.forward(dec_input, dec_hidden) \n",
    "            output_idx = torch.argmax(dec_output).data\n",
    "            outputs.append(int(output_idx))\n",
    "            \n",
    "            \n",
    "        return outputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "enc_vocab_size = input_size\n",
    "enc_embedding_dim = 100\n",
    "enc_hidden_dim = 50\n",
    "\n",
    "dec_vocab_size = output_size\n",
    "dec_embedding_dim = 100\n",
    "dec_hidden_dim = 50\n",
    "dec_output_dim = output_size\n",
    "\n",
    "enc = encoder(enc_vocab_size, enc_embedding_dim, enc_hidden_dim, batch_size=1)\n",
    "dec = decoder(dec_vocab_size, dec_embedding_dim, dec_hidden_dim, dec_output_dim, batch_size=1)\n",
    "s2s = seq2seq(enc, dec)\n",
    "\n",
    "\n",
    "optim = SGD(params=s2s.parameters(), lr=0.01)\n",
    "criterion = NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch|it: 0|100, Total Loss: 9.65\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c3fc42e6dc43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    s2s.train()\n",
    "    total_loss = 0\n",
    "    s2s.train()\n",
    "    for it, example in enumerate(train_data):\n",
    "\n",
    "        if (it % 100 == 0) and (it != 0):\n",
    "            print(\"Epoch|it: {}|{}, Total Loss: {:.2f}\".format(epoch, it, total_loss / it))\n",
    "        input_seq, output_seq = example\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_seq = torch.LongTensor(input_seq)\n",
    "        output_seq = torch.LongTensor(output_seq)\n",
    "\n",
    "        res = s2s.forward(input_seq, output_seq[:-1], p_tf=0.5)\n",
    "        loss = criterion(res, output_seq[1:])\n",
    "        loss.backward()\n",
    "        total_loss += loss.data.numpy()\n",
    "\n",
    "        optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['empresariales',\n",
       " 'quedan',\n",
       " 'cancelación',\n",
       " 'rübig',\n",
       " 'entrevistas',\n",
       " 'países',\n",
       " 'alimentaria',\n",
       " 'ocupar',\n",
       " 'culturas',\n",
       " 'hora',\n",
       " 'nombramientos',\n",
       " 'colosal',\n",
       " 'paulatina',\n",
       " 'constituyen',\n",
       " 'sombra',\n",
       " 'interferían',\n",
       " 'venganza',\n",
       " 'democratacristiana',\n",
       " 'cero',\n",
       " 'hiv',\n",
       " 'reintroducir']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_idx = output2idx['<SOS>']\n",
    "eos_idx = output2idx['<EOS>']\n",
    "pred_idxs = s2s.predict(input_seq, sos_idx, eos_idx)\n",
    "[idx2output[i] for i in pred_idxs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
